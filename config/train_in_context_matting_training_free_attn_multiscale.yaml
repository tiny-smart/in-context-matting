model:
  target: icm.models.in_context_matting_.InContextMatting
  params:
    learning_rate: 0.0004
    cfg_loss_function:
      target: icm.models.criterion.loss_function.LossFunction2
      params:
        losses_seg: [
            # "unknown_l1_loss",
            # "known_l1_loss",
            # "loss_pha_laplacian",
            # "loss_gradient_penalty",
            "known_smooth_l1_loss",
            # "cross_entropy_loss",
            # "focal_loss",
          ]
        losses_matting: [
            "unknown_l1_loss",
            "known_l1_loss",
            "loss_pha_laplacian",
            "loss_gradient_penalty",

          ]
    cfg_scheduler:
      target: icm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [250] # NOTE 1 for resuming. use 250 if starting from scratch
        cycle_lengths: [10000000000000]
        f_start: [1.e-6]
        f_max: [1.]
        f_min: [1.]

    cfg_feature_extractor:
      target: icm.src.models.dift_sd.FeatureExtractor
      params:
        sd_id: "pretrained_models/stable-diffusion-2-1"
        load_local: True
        if_softmax: 1
        feature_index_cor: 1
        feature_index_matting: [0, 1]
        attention_res: [24, 48] # [24, 48]
        set_diag_to_one: False
        time_steps: [200]
        extract_feature_inputted_to_layer: False
        ensemble_size: 4

    cfg_in_context_decoder:
      target: icm.models.decoder.in_context_decoder.InContextDecoder
      params:
        freeze_in_context_fusion: False # True for finetuning
        cfg_detail_decoder:
          target: icm.models.decoder.detail_capture.DetailCapture
          params:
            use_sigmoid: True
            ckpt: ""
            in_chans: 320
            img_chans: 3
            convstream_out: [48, 96, 192]
            fusion_out: [256, 128, 64, 32]
        cfg_in_context_fusion:
          target: icm.models.decoder.in_context_correspondence.SemiTrainingAttentionBlocks
          params:
            res_ratio: null
            pool_type: "min"
            upsample_mode: "bicubic"
            bottle_neck_dim: null
            use_norm: 1280
            in_ft_dim: [1280, 1280]
            in_attn_dim: [576, 2304]
            attn_out_dim: 256
            ft_out_dim: [320, 320]
            training_cross_attn: False

data:
  target: icm.data.data_module.DataModuleFromConfig
  params:
    batch_size: 2
    batch_size_val: 1
    num_workers: 8
    shuffle_train: False
    train:
      target: icm.data.data_generator.InContextDataset
      params:
        crop_size: 768
        phase: train
        norm_type: "sd" # "imagenet" or "sd"
        data:
          target: icm.data.image_file.ContextData
          params:
            ratio: 1
            dataset_name: [
                # "AIM-ICM",
                "open-images-test",
                "RM1k",
                # "PPM",
                "AM2k_train",
                "RWP636",
                "P3M_val_np",
                # "open-images-validation",
                # "open-images-train_0",
              ] # ["PPM", "AM2k", "RWP636", "P3M_val_np"]
    validation:
      target: icm.data.data_generator.InContextDataset
      params:
        crop_size: 768
        phase: val
        norm_type: "sd" # "imagenet" or "sd"
        data:
          target: icm.data.image_file.ContextData
          params:
            ratio: 0
            dataset_name: [
                "ICM57",
                # "open-images-test",
              ]

trainer:
  accelerator: "ddp"
  gpus: 2
  max_epochs: 1000
  auto_select_gpus: False
  num_sanity_val_steps: 0
  cfg_logger:
    target: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    params:
      save_dir: logs # set in main.py
      default_hp_metric: False
  cfg_callbacks:
    modelcheckpoint:
      target: icm.models.in_context_matting_.ModifiedModelCheckpoint
      params:
        monitor: epoch
        mode: max
        save_top_k: 100
        every_n_epochs: 1
        # save_last: True
        # save_weights_only: False
        filename: "{epoch:02d}-{val/mse_all:.5f}"
        auto_insert_metric_name: False
  plugins:
    target: pytorch_lightning.plugins.DDPPlugin
    params:
      find_unused_parameters: False
  # weights_summary: full

